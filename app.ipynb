{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text length: 208574 characters.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            all_text += doc.page_content + \"\\n\" \n",
    "\n",
    "print(f\"Total text length: {len(all_text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 690\n",
      "Sample chunk: Large Language Models: A Survey\n",
      "Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\n",
      "Richard Socher, Xavier Amatriain, Jianfeng Gao\n",
      "Abstract—Large Language Models (LLMs) have drawn a\n",
      "lot of attention due to their strong performance on a wide\n",
      "range of natural language tasks, since the release of ChatGPT\n",
      "in November 2022. LLMs’ ability of general-purpose language\n",
      "understanding and generation is acquired by training billions of\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks created: {len(chunks)}\")\n",
    "print(f\"Sample chunk: {chunks[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_docs = [Document(page_content=chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = Chroma.from_documents(hyde_docs, embedding=embedding, persist_directory=\"./chroma_db/HYDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "db3 = Chroma(persist_directory=\"./chroma_db/HYDE\", embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db3.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful AI assistant. Please answer the following question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Fine-tuining?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a helpful AI assistant. Please answer the following question:\\n\\nQuestion: What is Fine-tuining?\\n\\nAnswer:\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer = llm.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fine-tuning!\\n\\nFine-tuning refers to the process of adjusting or refining a pre-trained model, such as a neural network, to better fit a specific task or dataset. This technique involves using a small amount of labeled data and training algorithms to adapt the model's weights to the new task, without completely retraining it from scratch.\\n\\nIn other words, fine-tuning is like giving a model a gentle nudge in the right direction to help it learn new skills or improve its performance on a specific problem. This approach has been widely used in natural language processing (NLP) and computer vision tasks, where a pre-trained model can be fine-tuned for a particular domain or task, such as sentiment analysis, named entity recognition, or object detection.\\n\\nThe benefits of fine-tuning include:\\n\\n1. Reduced data requirements: Fine-tuning requires less labeled data compared to training from scratch.\\n2. Improved performance: By leveraging the knowledge learned by the pre-trained model, fine-tuning can lead to better results on the target task.\\n3. Faster development time: Fine-tuning allows developers to quickly adapt a model to a new task without starting from scratch.\\n\\nOverall, fine-tuning is a powerful technique for adapting pre-trained models to specific tasks and datasets, which has led to many state-of-the-art results in AI research!\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_answer = retriever.get_relevant_documents(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fine-tuning does not need to be performed to a single\\ntask though, and there are different approaches to multi-task\\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\\nor more tasks is known to improve results and reduce the\\ncomplexity of prompt engineering, and it can serve as an\\n2https://platform.openai.com/docs/guides/fine-tuning'),\n",
       " Document(page_content='perform specific tasks. In order for the foundation model to be\\nuseful it needed to be fine-tuned to a specific task with labeled\\ndata (so-called supervised fine-tuning or SFT for short). For\\nexample, in the original BERT paper [24], the model was fine-\\ntuned to 11 different tasks. While more recent LLMs no longer\\nrequire fine-tuning to be used, they can still benefit from task\\nor data-specific fine-tuning. For example, OpenAI reports that'),\n",
       " Document(page_content='Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270,\\n2022.\\n[132]\\nR. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\n“Parameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,” 2021.\\n[133]\\nS. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\\nT. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language\\nmodels: A survey,” 2023.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"\"\"\n",
    "You are a helpful AI assistant. Please answer the following question according to my context:\n",
    "\n",
    "question : {question}\n",
    "Context: {similar_answer}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\",\"similar_answer\"],\n",
    "    template=template1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(question=question,similar_answer=similar_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou are a helpful AI assistant. Please answer the following question according to my context:\\n\\nquestion : What is Fine-tuining?\\nContext: [Document(page_content='Fine-tuning does not need to be performed to a single\\\\ntask though, and there are different approaches to multi-task\\\\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\\\\nor more tasks is known to improve results and reduce the\\\\ncomplexity of prompt engineering, and it can serve as an\\\\n2https://platform.openai.com/docs/guides/fine-tuning'), Document(page_content='perform specific tasks. In order for the foundation model to be\\\\nuseful it needed to be fine-tuned to a specific task with labeled\\\\ndata (so-called supervised fine-tuning or SFT for short). For\\\\nexample, in the original BERT paper [24], the model was fine-\\\\ntuned to 11 different tasks. While more recent LLMs no longer\\\\nrequire fine-tuning to be used, they can still benefit from task\\\\nor data-specific fine-tuning. For example, OpenAI reports that'), Document(page_content='Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270,\\\\n2022.\\\\n[132]\\\\nR. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\\\n“Parameter-efficient multi-task fine-tuning for transformers via shared\\\\nhypernetworks,” 2021.\\\\n[133]\\\\nS. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\\\\nT. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language\\\\nmodels: A survey,” 2023.')]\\n\\nAnswer:\\n\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer_according_to_context  = llm.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, Fine-tuning refers to a process where a pre-trained foundation model is adapted or specialized to perform specific tasks by training it on labeled data. This involves adjusting the model's parameters to better suit the requirements of the target task(s), which can lead to improved results and reduced complexity in prompt engineering.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_answer_according_to_context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
